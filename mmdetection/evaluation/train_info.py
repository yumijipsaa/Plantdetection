import re
import torch
from mmengine import Config
from collections import OrderedDict

# infoê°€ í•„ìš”í•œ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
#log_file = 'work_dirs/casecade-mask-rcnn_r50_fpn_melon/20250305_160537/20250305_160537.log'
log_file = 'work_dirs/cascade-mask-rcnn_r50_fpn_onion/20250320_180829.log'

# ê²°ê³¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
log_data = {
    "total_loss": "N/A",
    "Training Time": "N/A",
    "GPU Model": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
    "loss_rpn_cls": "N/A",
    "epoch": "N/A",
    "learning rate": "N/A",
    "batch size": "N/A",
    "score_threshold": "N/A",
    "Dataset": "N/A",
    "Optimizer": "N/A"
}


# ğŸ”¹ 1ï¸âƒ£ Training Time ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€
total_training_time = None
num_iterations_per_epoch = 48  # 1 epochë‹¹ iteration ê°œìˆ˜ (ë¡œê·¸ì—ì„œ í™•ì¸ í•„ìš”)

# ğŸ”¹ 1ï¸âƒ£ ë¡œê·¸ íŒŒì¼ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ê°’ ì°¾ê¸°
with open(log_file, 'r') as f:
    logs = f.readlines()

# ì²« ë²ˆì§¸ë¡œ ë“±ì¥í•˜ëŠ” ë‚ ì§œ ì°¾ê¸° (YYYY/MM/DD í˜•ì‹)
for line in logs:
    match = re.search(r'(\d{4}/\d{2}/\d{2})', line)
    if match:
        log_data["Date"] = match.group(1)
          
    if "loss:" in line:
        match = re.search(r'loss:\s([\d.]+)', line)
        if match:
            log_data["total_loss"] = float(match.group(1))

    # ğŸ”¥ `Training Time` ì°¾ê¸° (ê°€ì¥ ë§ˆì§€ë§‰ `time:` ê°’ì„ ì‚¬ìš©)
    if "time:" in line:
        match = re.search(r'time:\s([\d.]+)', line)
        if match:
            iteration_time = float(match.group(1))
            log_data["Training Time"] = f"{iteration_time} sec per iteration"

            # ğŸ”¥ ì´ í•™ìŠµ ì‹œê°„ ê³„ì‚° (epoch ê°œìˆ˜ì™€ iteration ê°œìˆ˜ í™œìš©)
            if "epoch" in log_data and log_data["epoch"] != "N/A":
                total_training_time = iteration_time * log_data["epoch"] * num_iterations_per_epoch

    if "loss_rpn_cls:" in line:
        match = re.search(r'loss_rpn_cls:\s([\d.]+)', line)
        if match:
            log_data["loss_rpn_cls"] = float(match.group(1))

    if "Epoch(train)" in line:
        match = re.search(r'Epoch\(train\)\s+\[(\d+)', line)
        if match:
            log_data["epoch"] = int(match.group(1))

    if "lr:" in line:
        match = re.search(r'lr:\s([\d.e-]+)', line)
        if match:
            log_data["learning rate"] = float(match.group(1))



if total_training_time:
    hours = int(total_training_time // 3600)
    minutes = int((total_training_time % 3600) // 60)
    seconds = int(total_training_time % 60)
    log_data["Total Training Time"] = f"{hours}h {minutes}m {seconds}s"

# ğŸ”¹ 2ï¸âƒ£ `config.py`ì—ì„œ `score_threshold` ê°€ì ¸ì˜¤ê¸°
cfg = Config.fromfile('custom_configs/cascade-mask-rcnn_r50_fpn_onion.py')

if hasattr(cfg.model, 'test_cfg') and hasattr(cfg.model.test_cfg, 'rcnn'):
    log_data["score_threshold"] = cfg.model.test_cfg.rcnn.get('score_thr', "N/A")

# ğŸ”¹ 3ï¸âƒ£ `melon_instance.py` ì§ì ‘ ë¶ˆëŸ¬ì™€ì„œ `Dataset`, `batch_size`, `Optimizer` ê°€ì ¸ì˜¤ê¸°
data_cfg = Config.fromfile('custom_configs/_base_/datasets/onion_instance.py')

# Dataset ê°€ì ¸ì˜¤ê¸°
if hasattr(cfg, 'dataset_type'):
    log_data["Dataset"] = data_cfg.dataset_type

# Batch Size ê°€ì ¸ì˜¤ê¸°
if hasattr(data_cfg, 'train_dataloader') and 'batch_size' in data_cfg.train_dataloader:
    log_data["batch size"] = data_cfg.train_dataloader.batch_size

# `optim_wrapper` ì•ˆì—ì„œ `optimizer` ê°€ì ¸ì˜¤ê¸°
if hasattr(cfg, 'optim_wrapper') and hasattr(cfg.optim_wrapper, 'optimizer'):
    log_data["Optimizer"] = cfg.optim_wrapper.optimizer.get('type', "N/A")

# ğŸ”¹ 1ï¸âƒ£ ì‚¬ìš©í•œ ëª¨ë¸ê³¼ ë°±ë³¸ ì°¾ê¸°
if hasattr(cfg, 'model'):
    log_data["Model"] = cfg.model.get("type", "N/A")
    
    if hasattr(cfg.model, 'backbone'):
        backbone_type = cfg.model.backbone.get("type", "N/A")
        backbone_depth = cfg.model.backbone.get("depth", "")  # Depth ê°€ì ¸ì˜¤ê¸°
        log_data["Backbone"] = f"{backbone_type} {backbone_depth}".strip()


# `Date` â†’ `Model` â†’ `Backbone` ìˆœì„œë¡œ ì •ë ¬
ordered_log_data = OrderedDict([
    ("Date", log_data["Date"]),
    ("Model", log_data["Model"]),
    ("Backbone", log_data["Backbone"]),
    ("score_threshold", log_data["score_threshold"]),
    ("learning rate", log_data["learning rate"]),
    ("batch size", log_data["batch size"]),
    ("epoch", log_data["epoch"]),
    ("total_loss", log_data["total_loss"]),
    ("Total Training Time", log_data.get("Total Training Time", "N/A")),  # ğŸ”¥ ì´ í•™ìŠµ ì‹œê°„ ì¶”ê°€
    ("GPU Model", log_data["GPU Model"]),
    ("loss_rpn_cls", log_data["loss_rpn_cls"]),
    ("Dataset", log_data["Dataset"]),
    ("Optimizer", log_data["Optimizer"])
])

# ğŸ”¹ 4ï¸âƒ£ ìµœì¢… ì •ë ¬ëœ ê²°ê³¼ ì¶œë ¥
print("\nğŸ”¹ Extracted Training Information ğŸ”¹")
for key, value in ordered_log_data.items():
    print(f"{key}: {value}")


log_file_path = "evaluation_results/train_info/training_log.txt"

# íŒŒì¼ ì €ì¥
with open(log_file_path, "w", encoding="utf-8") as f:
    for key, value in ordered_log_data.items():
        f.write(f"{key}: {value}\n")

print(f"ë¡œê·¸ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {log_file_path}")



